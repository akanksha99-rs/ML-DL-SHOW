{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras import backend as K\n",
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_width, img_height = 28,28\n",
    "\n",
    "#defining the data directories\n",
    "train_data_dir= 'flowers/Train'\n",
    "validation_data_dir= 'flowers/Validation'\n",
    "n_training_sample= 400\n",
    "n_validation_sample= 100\n",
    "epochs=20\n",
    "batch_size=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if K.image_data_format() == 'channels_first':\n",
    "    input_shape = (3, img_width, img_height)\n",
    "else:\n",
    "    input_shape = (img_width, img_height, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Conv2D(64, (3,3), activation='relu', input_shape=(28, 28, 3)),\n",
    "  tf.keras.layers.MaxPooling2D(2, 2),\n",
    "  tf.keras.layers.Flatten(), \n",
    "  tf.keras.layers.Dense(2000, activation=tf.nn.sigmoid), \n",
    "                                    tf.keras.layers.Dense(1000, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(500, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(256, activation=tf.nn.relu), \n",
    "                                    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n",
    "                                    tf.keras.layers.Dense(5, activation=tf.nn.sigmoid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(),\n",
    "              loss = 'categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3578 images belonging to 5 classes.\n",
      "Found 400 images belonging to 5 classes.\n",
      "WARNING:tensorflow:From <ipython-input-7-3f935665dbdd>:29: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 40 steps, validate for 10 steps\n",
      "Epoch 1/20\n",
      "40/40 [==============================] - 12s 307ms/step - loss: 1.6672 - accuracy: 0.1725 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 2/20\n",
      "40/40 [==============================] - 10s 251ms/step - loss: 1.6094 - accuracy: 0.1750 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 3/20\n",
      "40/40 [==============================] - 10s 261ms/step - loss: 1.6094 - accuracy: 0.1900 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 4/20\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.6094 - accuracy: 0.1650 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 5/20\n",
      "40/40 [==============================] - 10s 240ms/step - loss: 1.6094 - accuracy: 0.1650 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 6/20\n",
      "40/40 [==============================] - 9s 233ms/step - loss: 1.6094 - accuracy: 0.1725 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 7/20\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.6094 - accuracy: 0.1475 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 8/20\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.6094 - accuracy: 0.1450 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 9/20\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.6094 - accuracy: 0.1558 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 10/20\n",
      "40/40 [==============================] - 9s 225ms/step - loss: 1.6094 - accuracy: 0.1575 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 11/20\n",
      "40/40 [==============================] - 9s 231ms/step - loss: 1.6094 - accuracy: 0.1633 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 12/20\n",
      "40/40 [==============================] - 9s 235ms/step - loss: 1.6094 - accuracy: 0.2125 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 13/20\n",
      "40/40 [==============================] - 9s 223ms/step - loss: 1.6094 - accuracy: 0.1525 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 14/20\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.6094 - accuracy: 0.1775 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 15/20\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.6094 - accuracy: 0.1525 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 16/20\n",
      "40/40 [==============================] - 9s 226ms/step - loss: 1.6094 - accuracy: 0.1750 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 17/20\n",
      "40/40 [==============================] - 9s 230ms/step - loss: 1.6094 - accuracy: 0.1675 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 18/20\n",
      "40/40 [==============================] - 10s 254ms/step - loss: 1.6094 - accuracy: 0.1475 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 19/20\n",
      "40/40 [==============================] - 9s 226ms/step - loss: 1.6094 - accuracy: 0.1875 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 20/20\n",
      "40/40 [==============================] - 9s 227ms/step - loss: 1.6094 - accuracy: 0.1550 - val_loss: 1.6094 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x262ad3ab208>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True)\n",
    "\n",
    "# this is the augmentation configuration we will use for testing:\n",
    "# only rescaling\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "    validation_data_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=n_training_sample // batch_size,\n",
    "    epochs=epochs,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=n_validation_sample // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         1.         0.99999964 1.         0.9999995 ]]\n"
     ]
    }
   ],
   "source": [
    "pred= image.load_img('flowers/Test/4804434999_bf2187e96a_n.jpg', target_size=(28,28))\n",
    "pred=image.img_to_array(pred)\n",
    "pred= np.expand_dims(pred, axis=0)\n",
    "result= model.predict(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
